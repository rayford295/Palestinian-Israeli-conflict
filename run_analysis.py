# -*- coding: utf-8 -*-
"""run_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15D2050IYyNv9wuAUMEc14KAlDgiIO8F6
"""

#关键词和词频统计
import pandas as pd
import jieba
import jieba.analyse
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer

# 定义一个函数来处理每个文件
def process_file(file_path, column_name):
    data = pd.read_csv(file_path)
    text = ' '.join(data[column_name].dropna())

    # 对于中文文本使用 jieba 进行关键词提取和词频统计
    if 'Chinese' in file_path:
        keywords = jieba.analyse.extract_tags(text, topK=10)
        words = jieba.lcut(text)
        word_freq = Counter(words).most_common(10)
    else:
        # 对于英文文本使用 TfidfVectorizer 进行关键词提取
        vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)
        tfidf_matrix = vectorizer.fit_transform([text])
        feature_array = vectorizer.get_feature_names_out()
        tfidf_sorting = tfidf_matrix.max(axis=0).toarray()[0].argsort()[::-1]
        keywords = feature_array[tfidf_sorting][:10]

        # 词频统计
        word_freq = pd.Series(text.split()).value_counts().head(10)

    print(f"File: {file_path}")
    print(f"Top 10 Keywords: {keywords}")
    print(f"Top 10 Word Frequency: {word_freq}")
    print("\n")

# 文件路径和对应的列名
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented'),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed'),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed'),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed')
]

# 处理每个文件
for file, column in files_columns:
    process_file(file, column)

#时序分析
import pandas as pd
import matplotlib.pyplot as plt

# 文件路径
file_path_chinese = '/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv'
file_path_english = '/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv'

# 加载数据
data_chinese = pd.read_csv(file_path_chinese)
data_english = pd.read_csv(file_path_english)

# 将 pubDate 转换为 datetime 格式
data_chinese['pubDate'] = pd.to_datetime(data_chinese['pubDate'])
data_english['pubDate'] = pd.to_datetime(data_english['pubDate'])

# 按月份进行分组并计数
monthly_counts_chinese = data_chinese.groupby(data_chinese['pubDate'].dt.to_period('M')).size()
monthly_counts_english = data_english.groupby(data_english['pubDate'].dt.to_period('M')).size()

print(monthly_counts_chinese)
print(monthly_counts_english)

# 绘制时间序列图
plt.figure(figsize=(12, 6))
monthly_counts_chinese.plot(label='Chinese News')
monthly_counts_english.plot(label='English News')
plt.title('Time Series Analysis of News Counts per Month')
plt.xlabel('Month')
plt.ylabel('Number of News Articles')
plt.legend()
plt.show()

#文本相似度分析
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(file_path, text_column):
    # 读取数据
    data = pd.read_csv(file_path)

    # 移除缺失值
    data = data.dropna(subset=[text_column])

    # 提取特定的文本列进行分析
    texts = data[text_column]

    # 计算 TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(texts)

    # 计算余弦相似度
    cosine_sim = cosine_similarity(tfidf_matrix)

    # 返回相似度矩阵
    return cosine_sim

# 文件路径和对应处理的文本列
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented'),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed'),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed'),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed')
]

# 对每个文件执行相似度分析
for file_path, text_column in files_columns:
    similarity_matrix = calculate_similarity(file_path, text_column)
    print(f"Similarity matrix for {file_path.split('/')[-1]}:")
    print(similarity_matrix)

import pandas as pd
from textblob import TextBlob
from snownlp import SnowNLP

# 定义一个函数来进行情感分析
def perform_sentiment_analysis(file_path, text_column, is_chinese=False):
    data = pd.read_csv(file_path)
    sentiments = []
    # 移除缺失值
    data = data.dropna(subset=[text_column])

    for text in data[text_column].dropna():
        if is_chinese:
            # 使用 SnowNLP 进行中文情感分析
            sentiment = SnowNLP(text).sentiments
        else:
            # 使用 TextBlob 进行英文情感分析
            sentiment = TextBlob(text).sentiment.polarity
        sentiments.append(sentiment)

    # 将情感分数添加到数据框中
    data['sentiment'] = sentiments
    return data

# 文件路径和对应处理的文本列
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented', True),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed', False),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed', True),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed', False)
]


# 对每个文件执行情感分析并保存结果
for file_path, text_column, is_chinese in files_columns:
    analyzed_data = perform_sentiment_analysis(file_path, text_column, is_chinese)

    # 输出结果到 CSV 文件
    output_file_path = file_path.replace("preprocessed_", "sentiment_analysis_")
    analyzed_data.to_csv(output_file_path, index=False)

    print(f"Sentiment analysis completed for {file_path.split('/')[-1]}")
    print(f"Results saved to {output_file_path}")

pip install SnowNLP

import pandas as pd
import gensim
import gensim.corpora as corpora
from gensim.models.ldamodel import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')


# 定义一个函数来读取数据并进行预处理
def load_and_preprocess(file_path, text_column):
    data = pd.read_csv(file_path)
    data = data.dropna(subset=[text_column])
    texts = data[text_column].apply(lambda x: word_tokenize(x.lower()))
    stop_words = set(stopwords.words('english'))
    texts = texts.apply(lambda x: [word for word in x if word not in stop_words])
    return texts

# 读取和预处理数据
file_paths = [
    '/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv',
    '/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv',
    '/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv',
    '/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'
]
text_columns = ['title_segmented', 'title_processed', 'content_processed', 'content_processed']

corpora_documents = []
for file_path, text_column in zip(file_paths, text_columns):
    texts = load_and_preprocess(file_path, text_column)
    corpora_documents.extend(texts)

# 创建字典和语料库
dictionary = corpora.Dictionary(corpora_documents)
corpus = [dictionary.doc2bow(text) for text in corpora_documents]

# 应用LDA主题建模
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)

# 打印主题
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
import string

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

def analyze_text_style(file_path, text_column):
    data = pd.read_csv(file_path)
    text = ' '.join(data[text_column].dropna())

    # Tokenization
    words = word_tokenize(text)
    sentences = sent_tokenize(text)

    # Remove punctuation and stopwords
    words = [word for word in words if word not in string.punctuation]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Calculating lexical richness
    lexical_richness = len(set(words)) / len(words)

    # Average sentence length
    avg_sentence_length = len(words) / len(sentences)

    # Most common words and phrases
    freq_dist = FreqDist(words)
    most_common_words = freq_dist.most_common(10)

    # Part of speech distribution
    pos_tags = nltk.pos_tag(words)
    pos_counts = nltk.FreqDist(tag for (word, tag) in pos_tags)

    return {
        "lexical_richness": lexical_richness,
        "avg_sentence_length": avg_sentence_length,
        "most_common_words": most_common_words,
        "pos_distribution": pos_counts
    }

# 文件路径和对应处理的文本列
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented'),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed'),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed'),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed')
]

# 对每个文件进行语言风格分析
for file_path, text_column in files_columns:
    style_analysis = analyze_text_style(file_path, text_column)
    print(f"Language style analysis for {file_path.split('/')[-1]}:")
    print(style_analysis)

import pandas as pd
import jieba
import jieba.analyse
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from textblob import TextBlob
from snownlp import SnowNLP
import pandas as pd
import gensim
import gensim.corpora as corpora
from gensim.models.ldamodel import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
import string

# 分析一关键词和词频统计
# 定义一个函数来处理每个文件
def process_file(file_path, column_name):
    data = pd.read_csv(file_path)
    text = ' '.join(data[column_name].dropna())

    # 对于中文文本使用 jieba 进行关键词提取和词频统计
    if 'Chinese' in file_path:
        keywords = jieba.analyse.extract_tags(text, topK=10)
        words = jieba.lcut(text)
        word_freq = Counter(words).most_common(10)
    else:
        # 对于英文文本使用 TfidfVectorizer 进行关键词提取
        vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)
        tfidf_matrix = vectorizer.fit_transform([text])
        feature_array = vectorizer.get_feature_names_out()
        tfidf_sorting = tfidf_matrix.max(axis=0).toarray()[0].argsort()[::-1]
        keywords = feature_array[tfidf_sorting][:10]

        # 词频统计
        word_freq = pd.Series(text.split()).value_counts().head(10)

    print(f"File: {file_path}")
    print(f"Top 10 Keywords: {keywords}")
    print(f"Top 10 Word Frequency: {word_freq}")
    print("\n")

# 文件路径和对应的列名
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented'),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed'),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed'),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed')
]

# 处理每个文件
for file, column in files_columns:
    process_file(file, column)

#分析二时序分析
import pandas as pd
import matplotlib.pyplot as plt

# 文件路径
file_path_chinese = '/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv'
file_path_english = '/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv'

# 加载数据
data_chinese = pd.read_csv(file_path_chinese)
data_english = pd.read_csv(file_path_english)

# 将 pubDate 转换为 datetime 格式
data_chinese['pubDate'] = pd.to_datetime(data_chinese['pubDate'])
data_english['pubDate'] = pd.to_datetime(data_english['pubDate'])

# 按月份进行分组并计数
monthly_counts_chinese = data_chinese.groupby(data_chinese['pubDate'].dt.to_period('M')).size()
monthly_counts_english = data_english.groupby(data_english['pubDate'].dt.to_period('M')).size()

print(monthly_counts_chinese)
print(monthly_counts_english)

# 绘制时间序列图
plt.figure(figsize=(12, 6))
monthly_counts_chinese.plot(label='Chinese News')
monthly_counts_english.plot(label='English News')
plt.title('Time Series Analysis of News Counts per Month')
plt.xlabel('Month')
plt.ylabel('Number of News Articles')
plt.legend()
plt.show()

#分析三文本相似度分析
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(file_path, text_column):
    # 读取数据
    data = pd.read_csv(file_path)

    # 移除缺失值
    data = data.dropna(subset=[text_column])

    # 提取特定的文本列进行分析
    texts = data[text_column]

    # 计算 TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(texts)

    # 计算余弦相似度
    cosine_sim = cosine_similarity(tfidf_matrix)

    # 返回相似度矩阵
    return cosine_sim

# 文件路径和对应处理的文本列
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented'),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed'),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed'),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed')
]

# 对每个文件执行相似度分析
for file_path, text_column in files_columns:
    similarity_matrix = calculate_similarity(file_path, text_column)
    print(f"Similarity matrix for {file_path.split('/')[-1]}:")
    print(similarity_matrix)

import pandas as pd
from textblob import TextBlob
from snownlp import SnowNLP
# 分析四，情感分析
# 定义一个函数来进行情感分析
def perform_sentiment_analysis(file_path, text_column, is_chinese=False):
    data = pd.read_csv(file_path)
    sentiments = []
    # 移除缺失值
    data = data.dropna(subset=[text_column])

    for text in data[text_column].dropna():
        if is_chinese:
            # 使用 SnowNLP 进行中文情感分析
            sentiment = SnowNLP(text).sentiments
        else:
            # 使用 TextBlob 进行英文情感分析
            sentiment = TextBlob(text).sentiment.polarity
        sentiments.append(sentiment)

    # 将情感分数添加到数据框中
    data['sentiment'] = sentiments
    return data

# 文件路径和对应处理的文本列
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented', True),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed', False),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed', True),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed', False)
]


# 对每个文件执行情感分析并保存结果
for file_path, text_column, is_chinese in files_columns:
    analyzed_data = perform_sentiment_analysis(file_path, text_column, is_chinese)

    # 输出结果到 CSV 文件
    output_file_path = file_path.replace("preprocessed_", "sentiment_analysis_")
    analyzed_data.to_csv(output_file_path, index=False)

    print(f"Sentiment analysis completed for {file_path.split('/')[-1]}")
    print(f"Results saved to {output_file_path}")

import pandas as pd
import gensim
import gensim.corpora as corpora
from gensim.models.ldamodel import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')

# 分析五主题建模分析
# 定义一个函数来读取数据并进行预处理
def load_and_preprocess(file_path, text_column):
    data = pd.read_csv(file_path)
    data = data.dropna(subset=[text_column])
    texts = data[text_column].apply(lambda x: word_tokenize(x.lower()))
    stop_words = set(stopwords.words('english'))
    texts = texts.apply(lambda x: [word for word in x if word not in stop_words])
    return texts

# 读取和预处理数据
file_paths = [
    '/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv',
    '/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv',
    '/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv',
    '/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'
]
text_columns = ['title_segmented', 'title_processed', 'content_processed', 'content_processed']

corpora_documents = []
for file_path, text_column in zip(file_paths, text_columns):
    texts = load_and_preprocess(file_path, text_column)
    corpora_documents.extend(texts)

# 创建字典和语料库
dictionary = corpora.Dictionary(corpora_documents)
corpus = [dictionary.doc2bow(text) for text in corpora_documents]

# 应用LDA主题建模
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)

# 打印主题
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
import string

#分析六语言风格分析
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

def analyze_text_style(file_path, text_column):
    data = pd.read_csv(file_path)
    text = ' '.join(data[text_column].dropna())

    # Tokenization
    words = word_tokenize(text)
    sentences = sent_tokenize(text)

    # Remove punctuation and stopwords
    words = [word for word in words if word not in string.punctuation]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Calculating lexical richness
    lexical_richness = len(set(words)) / len(words)

    # Average sentence length
    avg_sentence_length = len(words) / len(sentences)

    # Most common words and phrases
    freq_dist = FreqDist(words)
    most_common_words = freq_dist.most_common(10)

    # Part of speech distribution
    pos_tags = nltk.pos_tag(words)
    pos_counts = nltk.FreqDist(tag for (word, tag) in pos_tags)

    return {
        "lexical_richness": lexical_richness,
        "avg_sentence_length": avg_sentence_length,
        "most_common_words": most_common_words,
        "pos_distribution": pos_counts
    }

# 文件路径和对应处理的文本列
files_columns = [
    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'title_segmented'),
    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'title_processed'),
    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed'),
    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'content_processed')
]

# 对每个文件进行语言风格分析
for file_path, text_column in files_columns:
    style_analysis = analyze_text_style(file_path, text_column)
    print(f"Language style analysis for {file_path.split('/')[-1]}:")
    print(style_analysis)