{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install SnowNLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc1R7HeRpboY",
        "outputId": "fb7b055e-1767-4f0f-b051-22668915d4aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SnowNLP\n",
            "  Downloading snownlp-0.12.3.tar.gz (37.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: SnowNLP\n",
            "  Building wheel for SnowNLP (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SnowNLP: filename=snownlp-0.12.3-py3-none-any.whl size=37760944 sha256=586b372131469558d2127ec2c96ab6458e0f918edd39bf2c040243f7bc0a65a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/f3/70/8990fc249efeb396007766676706f71dd3d1ca3c023ce522ce\n",
            "Successfully built SnowNLP\n",
            "Installing collected packages: SnowNLP\n",
            "Successfully installed SnowNLP-0.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from textblob import TextBlob\n",
        "from snownlp import SnowNLP\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Yifan Analysis 1: Keyword and Word Frequency Analysis\n",
        "def yifan_process_file(file_path, column_name):\n",
        "    data = pd.read_csv(file_path)\n",
        "    text = ' '.join(data[column_name].dropna())\n",
        "    # Chinese text analysis\n",
        "    if 'Chinese' in file_path:\n",
        "        keywords = jieba.analyse.extract_tags(text, topK=10)\n",
        "        words = jieba.lcut(text)\n",
        "        word_freq = Counter(words).most_common(10)\n",
        "    else:\n",
        "        # English text analysis\n",
        "        vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "        tfidf_matrix = vectorizer.fit_transform([text])\n",
        "        feature_array = vectorizer.get_feature_names_out()\n",
        "        tfidf_sorting = tfidf_matrix.max(axis=0).toarray()[0].argsort()[::-1]\n",
        "        keywords = feature_array[tfidf_sorting][:10]\n",
        "        word_freq = pd.Series(text.split()).value_counts().head(10)\n",
        "\n",
        "    return {\"Top 10 Keywords\": keywords, \"Top 10 Word Frequency\": word_freq}\n",
        "\n",
        "# Yifan Analysis 2: Time Series Analysis\n",
        "def yifan_time_series_analysis(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    if 'pubDate' in data.columns:\n",
        "        data['pubDate'] = pd.to_datetime(data['pubDate'])\n",
        "        monthly_counts = data.groupby(data['pubDate'].dt.to_period('M')).size()\n",
        "        return monthly_counts\n",
        "    else:\n",
        "        print(f\"'pubDate' column not found in {file_path}\")\n",
        "        return None\n",
        "\n",
        "# Yifan Analysis 3: Text Similarity Analysis\n",
        "def yifan_calculate_similarity(file_path, text_column):\n",
        "    data = pd.read_csv(file_path).dropna(subset=[text_column])\n",
        "    texts = data[text_column]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Yifan Analysis 4: Sentiment Analysis\n",
        "def yifan_perform_sentiment_analysis(file_path, text_column, is_chinese):\n",
        "    data = pd.read_csv(file_path).dropna(subset=[text_column])\n",
        "    sentiments = [SnowNLP(text).sentiments if is_chinese else TextBlob(text).sentiment.polarity for text in data[text_column]]\n",
        "    data['sentiment'] = sentiments\n",
        "    return data\n",
        "\n",
        "# Yifan Analysis 5: Topic Modeling Analysis\n",
        "def yifan_topic_modeling_analysis(file_path, text_column):\n",
        "    data = pd.read_csv(file_path).dropna(subset=[text_column])\n",
        "    texts = data[text_column].apply(lambda x: word_tokenize(x.lower()))\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    texts = texts.apply(lambda x: [word for word in x if word not in stop_words])\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "    return lda_model.print_topics(-1)\n",
        "\n",
        "# Yifan Analysis 6: Language Style Analysis\n",
        "def yifan_analyze_text_style(file_path, text_column):\n",
        "    data = pd.read_csv(file_path)\n",
        "    text = ' '.join(data[text_column].dropna())\n",
        "    words = word_tokenize(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word for word in words if word not in string.punctuation]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    lexical_richness = len(set(words)) / len(words)\n",
        "    avg_sentence_length = len(words) / len(sentences)\n",
        "    freq_dist = FreqDist(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    pos_counts = nltk.FreqDist(tag for (word, tag) in pos_tags)\n",
        "\n",
        "    # Prepare the data for DataFrame\n",
        "    data_for_df = {\n",
        "        \"lexical_richness\": [lexical_richness],\n",
        "        \"avg_sentence_length\": [avg_sentence_length],\n",
        "        \"most_common_words\": [dict(freq_dist.most_common(10))],\n",
        "        \"pos_distribution\": [dict(pos_counts)]\n",
        "    }\n",
        "\n",
        "    return data_for_df\n",
        "\n",
        "\n",
        "# Main execution block，very important\n",
        "files_columns = [\n",
        "    ('/content/preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv', 'text_processed', True),\n",
        "    ('/content/preprocessed_English_version_of_Google_News_crawling_yifan.csv', 'text_processed', False),\n",
        "    ('/content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'text_processed', True),\n",
        "    ('/content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv', 'text_processed', False)\n",
        "]\n",
        "\n",
        "for file_path, text_column, is_chinese in files_columns:\n",
        "    keyword_freq_data = yifan_process_file(file_path, text_column)  # Analysis 1\n",
        "    pd.DataFrame(keyword_freq_data).to_csv(file_path.replace(\"preprocessed_\", \"keyword_freq_analysis_\"), index=False)\n",
        "\n",
        "    monthly_counts = yifan_time_series_analysis(file_path)  # Analysis 2\n",
        "    if monthly_counts is not None:\n",
        "        monthly_counts.to_csv(file_path.replace(\"preprocessed_\", \"time_series_analysis_\"), index=True)\n",
        "\n",
        "    similarity_matrix = yifan_calculate_similarity(file_path, text_column)  # Analysis 3\n",
        "    pd.DataFrame(similarity_matrix).to_csv(file_path.replace(\"preprocessed_\", \"similarity_analysis_\"), index=False)\n",
        "\n",
        "    sentiment_data = yifan_perform_sentiment_analysis(file_path, text_column, is_chinese)  # Analysis 4\n",
        "    sentiment_output_file = file_path.replace(\"preprocessed_\", \"sentiment_analysis_\")\n",
        "    sentiment_data.to_csv(sentiment_output_file, index=False)\n",
        "\n",
        "    topics = yifan_topic_modeling_analysis(file_path, text_column)  # Analysis 5\n",
        "    pd.DataFrame({\"Topics\": topics}).to_csv(file_path.replace(\"preprocessed_\", \"topic_modeling_analysis_\"), index=False)\n",
        "\n",
        "    style_analysis = yifan_analyze_text_style(file_path, text_column)  # Analysis 6\n",
        "    pd.DataFrame(style_analysis, index=[0]).to_csv(file_path.replace(\"preprocessed_\", \"style_analysis_\"), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzxtsgFRwrP3",
        "outputId": "e7ffc9f8-0c30-4f4b-ec81-71614911e643"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "<ipython-input-11-654e1178499f>:49: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
            "  monthly_counts = data.groupby(data['pubDate'].dt.to_period('M')).size()\n",
            "<ipython-input-11-654e1178499f>:49: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
            "  monthly_counts = data.groupby(data['pubDate'].dt.to_period('M')).size()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'pubDate' column not found in /content/preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv\n",
            "'pubDate' column not found in /content/preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv\n"
          ]
        }
      ]
    }
  ]
}