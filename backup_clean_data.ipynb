{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def yifan_clean_chinese_text(text):\n",
        "    \"\"\"\n",
        "    Cleans Chinese text by keeping only Chinese characters.\n",
        "    \"\"\"\n",
        "    return ''.join(re.findall(r'[\\u4e00-\\u9fff]+', text))\n",
        "\n",
        "def yifan_segment_text(text, stopwords, language='Chinese'):\n",
        "    \"\"\"\n",
        "    Segments text using jieba for Chinese or simple split for English and removes stopwords.\n",
        "    \"\"\"\n",
        "    if language == 'Chinese':\n",
        "        words = jieba.cut(text)\n",
        "    else:  # English\n",
        "        words = text.split()\n",
        "    return ' '.join(word for word in words if word not in stopwords)\n",
        "\n",
        "def yifan_process_dataset(file_path, language, stopwords):\n",
        "    \"\"\"\n",
        "    Processes the dataset by reading, cleaning, segmenting, and creating a TF-IDF matrix.\n",
        "    Returns the processed DataFrame and the TF-IDF matrix.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "    text_column = 'title' if 'Google_News' in file_path else 'Content'\n",
        "\n",
        "    # Ensure the column is treated as string\n",
        "    data[text_column] = data[text_column].astype(str)\n",
        "\n",
        "    if language == 'Chinese':\n",
        "        data['text_cleaned'] = data[text_column].apply(yifan_clean_chinese_text)\n",
        "    else:  # English\n",
        "        data['text_cleaned'] = data[text_column].apply(lambda x: x.lower())\n",
        "        data['text_cleaned'] = data['text_cleaned'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "    data['text_processed'] = data['text_cleaned'].apply(lambda x: yifan_segment_text(x, stopwords, language))\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(data['text_processed'])\n",
        "    return data, tfidf_matrix\n",
        "\n",
        "# Define stopwords for Chinese and English\n",
        "yifan_chinese_stopwords = set([\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\"])\n",
        "yifan_english_stopwords = set(['the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'as', 'by', 'that', 'this', 'it', 'are', 'at', 'be', 'from', 'or', 'an', 'was', 'will', 'not', 'have', 'has', 'had', 'its', 'it\\'s', 'you', 'your', 'they', 'their', 'what', 'which'])\n",
        "\n",
        "# Process datasets\n",
        "file_path1 = '/content/Chinese_version_of_Google_News_crawling_yifan.csv'\n",
        "data1, tfidf_matrix1 = yifan_process_dataset(file_path1, 'Chinese', yifan_chinese_stopwords)\n",
        "output_path1 = 'preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv'\n",
        "data1.to_csv(output_path1, index=False)\n",
        "\n",
        "# English_version_of_Google_News_crawling_yifan.csv\n",
        "file_path2 = '/content/English_version_of_Google_News_crawling_yifan.csv'\n",
        "data2, tfidf_matrix2 = yifan_process_dataset(file_path2, 'English', yifan_english_stopwords)\n",
        "output_path2 = 'preprocessed_English_version_of_Google_News_crawling_yifan.csv'\n",
        "data2.to_csv(output_path2, index=False)\n",
        "\n",
        "# Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv\n",
        "file_path3 = '/content/Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data3, tfidf_matrix3 = yifan_process_dataset(file_path3, 'Chinese', yifan_chinese_stopwords)\n",
        "output_path3 = 'preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data3.to_csv(output_path3, index=False)\n",
        "\n",
        "# Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv\n",
        "file_path4 = '/content/Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data4, tfidf_matrix4 = yifan_process_dataset(file_path4, 'English', yifan_english_stopwords)\n",
        "output_path4 = 'preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data4.to_csv(output_path4, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "wMuGRSoTYY6Z"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}