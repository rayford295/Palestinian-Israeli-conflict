{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 加载数据\n",
        "file_path = '/content/Chinese_version_of_Google_News_crawling_yifan.csv'  # 替换为您的 CSV 文件路径\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 清理文本：只保留汉字\n",
        "data['title_cleaned'] = data['title'].apply(lambda x: ''.join(re.findall(r'[\\u4e00-\\u9fff]+', x)))\n",
        "\n",
        "# 使用 jieba 进行分词\n",
        "data['title_segmented'] = data['title_cleaned'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
        "\n",
        "# 停用词表\n",
        "stopwords = set([\n",
        "    \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\",\n",
        "    \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\",\n",
        "    \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\",\n",
        "    \"看\", \"好\", \"自己\", \"这\"\n",
        "    # 可根据需要添加更多停用词\n",
        "])\n",
        "\n",
        "# 去除停用词\n",
        "data['title_segmented'] = data['title_segmented'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords))\n",
        "\n",
        "# 将文本转换为 TF-IDF 向量（这里不会实际输出向量，只是作为数据处理的一部分）\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data['title_segmented'])\n",
        "\n",
        "# 保存预处理后的数据到 CSV\n",
        "output_path = 'preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv'  # 您希望保存的文件路径\n",
        "data.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "HLR_aIlOG7dN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 加载数据\n",
        "file_path = '/content/English_version_of_Google_News_crawling_yifan.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 文本预处理\n",
        "# 1. 转换为小写\n",
        "data['title_cleaned'] = data['title'].apply(lambda x: x.lower())\n",
        "\n",
        "# 2. 移除特殊字符\n",
        "data['title_cleaned'] = data['title_cleaned'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "# 3. 简单分词\n",
        "data['title_tokenized'] = data['title_cleaned'].apply(lambda x: x.split())\n",
        "\n",
        "# 4. 基本停用词去除\n",
        "simple_stopwords = {'the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'as', 'by', 'that', 'this', 'it', 'are', 'at', 'be', 'from', 'or', 'an', 'was', 'will', 'not', 'have', 'has', 'had', 'its', 'it\\'s', 'you', 'your', 'they', 'their', 'what', 'which'}\n",
        "data['title_processed'] = data['title_tokenized'].apply(lambda x: [word for word in x if word not in simple_stopwords])\n",
        "\n",
        "# 5. 重新组合为字符串\n",
        "data['title_processed'] = data['title_processed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# 将文本转换为 TF-IDF 向量\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data['title_processed'])\n",
        "\n",
        "# 保存预处理后的数据到 CSV 文件\n",
        "output_path = 'preprocessed_English_version_of_Google_News_crawling_yifan.csv'\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dPZni0ZnHCNe",
        "outputId": "84f43021-f80a-462a-98d6-e931875687ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'preprocessed_English_version_of_Google_News_crawling_yifan.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "\n",
        "# 加载数据\n",
        "file_path = '/content/Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv'  # 替换为您的文件路径\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 清理文本：只保留汉字\n",
        "data['content_cleaned'] = data['Content'].apply(lambda x: ''.join(re.findall(r'[\\u4e00-\\u9fff]+', x)))\n",
        "\n",
        "# 使用 jieba 进行分词\n",
        "data['content_segmented'] = data['content_cleaned'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
        "\n",
        "# 使用基础停用词表去除停用词\n",
        "chinese_stopwords = set([\n",
        "    \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\",\n",
        "    \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\",\n",
        "    \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\",\n",
        "    \"看\", \"好\", \"自己\", \"这\"\n",
        "    # 可以根据需要添加更多停用词\n",
        "])\n",
        "\n",
        "data['content_processed'] = data['content_segmented'].apply(lambda x: ' '.join(word for word in x.split() if word not in chinese_stopwords))\n",
        "\n",
        "# 保存预处理后的数据到 CSV 文件\n",
        "output_path = 'preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv'  # 指定输出文件的路径\n",
        "data.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "EPTQFUVfJTIX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 加载数据\n",
        "file_path = '/content/Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 确保所有内容都是字符串类型\n",
        "data['Content'] = data['Content'].astype(str)\n",
        "\n",
        "# 硬编码的简单英文停用词列表\n",
        "simple_english_stopwords = set([\n",
        "    'the', 'and', 'is', 'in', 'to', 'of', 'a', 'for',\n",
        "    'on', 'with', 'as', 'by', 'that', 'this', 'it',\n",
        "    'are', 'at', 'be', 'from', 'or', 'an', 'was',\n",
        "    'will', 'not', 'have', 'has', 'had', 'its',\n",
        "    'it\\'s', 'you', 'your', 'they', 'their', 'what',\n",
        "    'which'\n",
        "])\n",
        "\n",
        "# 文本预处理\n",
        "# 1. 转换为小写\n",
        "data['content_cleaned'] = data['Content'].apply(lambda x: x.lower())\n",
        "\n",
        "# 2. 移除特殊字符\n",
        "data['content_cleaned'] = data['content_cleaned'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "# 3. 简单分词\n",
        "data['content_tokenized'] = data['content_cleaned'].apply(lambda x: x.split())\n",
        "\n",
        "# 4. 去除停用词\n",
        "data['content_processed'] = data['content_tokenized'].apply(lambda x: [word for word in x if word not in simple_english_stopwords])\n",
        "\n",
        "# 5. 重新组合为字符串\n",
        "data['content_processed'] = data['content_processed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# 保存预处理后的数据到 CSV 文件\n",
        "output_path = 'preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "9Ie_J5ZlJrCA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 处理第一个数据集：Chinese_version_of_Google_News_crawling_yifan.csv\n",
        "file_path1 = '/content/Chinese_version_of_Google_News_crawling_yifan.csv'\n",
        "data1 = pd.read_csv(file_path1)\n",
        "data1['title_cleaned'] = data1['title'].apply(lambda x: ''.join(re.findall(r'[\\u4e00-\\u9fff]+', x)))\n",
        "data1['title_segmented'] = data1['title_cleaned'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
        "stopwords1 = set([\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\"])\n",
        "data1['title_segmented'] = data1['title_segmented'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords1))\n",
        "vectorizer1 = TfidfVectorizer()\n",
        "tfidf_matrix1 = vectorizer1.fit_transform(data1['title_segmented'])\n",
        "output_path1 = 'preprocessed_Chinese_version_of_Google_News_crawling_yifan.csv'\n",
        "data1.to_csv(output_path1, index=False)\n",
        "\n",
        "# ...\n",
        "# 处理第二个数据集：English_version_of_Google_News_crawling_yifan.csv\n",
        "file_path2 = '/content/English_version_of_Google_News_crawling_yifan.csv'\n",
        "data2 = pd.read_csv(file_path2)\n",
        "data2['title_cleaned'] = data2['title'].apply(lambda x: x.lower())\n",
        "data2['title_cleaned'] = data2['title_cleaned'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "data2['title_tokenized'] = data2['title_cleaned'].apply(lambda x: x.split())\n",
        "simple_stopwords2 = {'the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'as', 'by', 'that', 'this', 'it', 'are', 'at', 'be', 'from', 'or', 'an', 'was', 'will', 'not', 'have', 'has', 'had', 'its', 'it\\'s', 'you', 'your', 'they', 'their', 'what', 'which'}\n",
        "data2['title_processed'] = data2['title_tokenized'].apply(lambda x: ' '.join([word for word in x if word not in simple_stopwords2]))\n",
        "vectorizer2 = TfidfVectorizer()\n",
        "tfidf_matrix2 = vectorizer2.fit_transform(data2['title_processed'])\n",
        "output_path2 = 'preprocessed_English_version_of_Google_News_crawling_yifan.csv'\n",
        "data2.to_csv(output_path2, index=False)\n",
        "# ...\n",
        "\n",
        "\n",
        "# 处理第三个数据集：Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv\n",
        "file_path3 = '/content/Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data3 = pd.read_csv(file_path3)\n",
        "data3['content_cleaned'] = data3['Content'].apply(lambda x: ''.join(re.findall(r'[\\u4e00-\\u9fff]+', x)))\n",
        "data3['content_segmented'] = data3['content_cleaned'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
        "chinese_stopwords3 = set([\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \"都\", \"一\", \"一个\", \"上\", \"也\", \"很\", \"到\", \"说\", \"要\", \"去\", \"你\", \"会\", \"着\", \"没有\", \"看\", \"好\", \"自己\", \"这\"])\n",
        "data3['content_processed'] = data3['content_segmented'].apply(lambda x: ' '.join(word for word in x.split() if word not in chinese_stopwords3))\n",
        "output_path3 = 'preprocessed_Wikipedia_Chinese_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data3.to_csv(output_path3, index=False)\n",
        "\n",
        "# 处理第四个数据集：Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv\n",
        "file_path4 = '/content/Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data4 = pd.read_csv(file_path4)\n",
        "data4['Content'] = data4['Content'].astype(str)\n",
        "simple_english_stopwords4 = set(['the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'as', 'by', 'that', 'this', 'it', 'are', 'at', 'be', 'from', 'or', 'an', 'was', 'will', 'not', 'have', 'has', 'had', 'its', 'it\\'s', 'you', 'your', 'they', 'their', 'what', 'which'])\n",
        "data4['content_cleaned'] = data4['Content'].apply(lambda x: x.lower())\n",
        "data4['content_cleaned'] = data4['content_cleaned'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "data4['content_tokenized'] = data4['content_cleaned'].apply(lambda x: x.split())\n",
        "data4['content_processed'] = data4['content_tokenized'].apply(lambda x: [word for word in x if word not in simple_english_stopwords4])\n",
        "data4['content_processed'] = data4['content_processed'].apply(lambda x: ' '.join(x))\n",
        "output_path4 = 'preprocessed_Wikipedia_English_version_of_the_Palestinian-Israeli_conflict_yifan.csv'\n",
        "data4.to_csv(output_path4, index=False)\n"
      ],
      "metadata": {
        "id": "uoJPDb7JMlWB"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}