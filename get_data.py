# -*- coding: utf-8 -*-
"""Untitled31.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13o9xrjdLFA6CDYYI_tMNz7FDlnP-f4AC
"""

import csv
import requests
from bs4 import BeautifulSoup

# 定义要爬取的网页列表
urls = {
    "Wikipedia_English": "https://en.wikipedia.org/wiki/Israeli%E2%80%93Palestinian_conflict",
    "Wikipedia_Chinese": "https://zh.wikipedia.org/wiki/%E4%BB%A5%E5%B7%B4%E5%86%B2%E7%AA%81",
    "Baidu_Chinese": "https://baike.baidu.com/item/%E5%B7%B4%E4%BB%A5%E5%86%B2%E7%AA%81?fromModule=lemma_search-box"
}

# 遍历网页列表，对每个网页执行爬取操作
for name, url in urls.items():
    # 发送HTTP请求获取网页内容
    response = requests.get(url)
    # 检查请求是否成功
    if response.status_code == 200:
        # 使用BeautifulSoup解析HTML内容
        soup = BeautifulSoup(response.content, 'html.parser')
        # 查找所有标题
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        # 准备写入CSV文件
        filename = f"{name}_yifan.csv"
        with open(filename, 'w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            # 写入CSV文件头
            writer.writerow(['Title', 'Content'])
            for heading in headings:
                # 获取标题文本
                title = heading.get_text().strip()
                content = []
                # 获取标题下的所有内容，直到下一个标题
                for sibling in heading.next_siblings:
                    if sibling.name and sibling.name.startswith('h'):
                        break
                    if sibling.name == 'p':
                        content.append(sibling.get_text().strip())
                # 写入标题和内容到CSV
                writer.writerow([title, ' '.join(content)])
        print(f"Content from {url} has been written to {filename}")
    else:
        print(f"Request to {url} failed with status code:", response.status_code)